# PagedAttention: Efficient Memory Management for LLM Inference

A from-scratch PyTorch implementation of PagedAttention and vLLM-like memory management for efficient KV cache handling in transformer inference.

## ğŸ“– Overview

This project implements the core ideas from the PagedAttention paper:

- **Block-based KV cache**: Split KV cache into fixed-size blocks (pages)
- **Non-contiguous memory**: Blocks can be stored anywhere in memory
- **Copy-on-Write (COW)**: Efficient memory sharing for beam search and parallel sampling
- **Swap and recompute**: Trade computation for memory when needed
- **Reduced fragmentation**: Near-zero memory waste compared to naive contiguous allocation

## ğŸš€ Quick Start

### Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Running the Project

```bash
# Run correctness tests
pytest tests/ -v

# Run demo (shows correctness and basic functionality)
python scripts/run_demo.py

# Run comprehensive benchmarks
python scripts/run_benchmarks.py

# Open interactive notebook
jupyter notebook demo_notebook.ipynb
```

## ğŸ“ Project Structure

```
paged_attention_project/
â”œâ”€â”€ paged_attention/          # Core implementation
â”‚   â”œâ”€â”€ paged_attention.py    # Blockwise attention module
â”‚   â”œâ”€â”€ kv_cache.py           # KV cache with block management
â”‚   â”œâ”€â”€ allocator.py          # Physical block allocator
â”‚   â”œâ”€â”€ scheduler.py          # Batch scheduler
â”‚   â”œâ”€â”€ decoding.py           # COW logic for sampling
â”‚   â”œâ”€â”€ swap_recompute.py     # Swap/recompute strategies
â”‚   â””â”€â”€ utils.py              # Utilities and plotting
â”œâ”€â”€ tests/                    # Unit tests
â”œâ”€â”€ benchmarks/               # Performance benchmarks
â”œâ”€â”€ scripts/                  # Demo and benchmark runners
â””â”€â”€ demo_notebook.ipynb       # Interactive experiments
```

## ğŸ¯ Key Features

- âœ… **Correctness verified**: Outputs match vanilla attention within 1e-5 tolerance
- âœ… **Memory efficient**: 60-80% memory savings vs naive allocation
- âœ… **Parallel sampling**: Share prompt blocks across samples
- âœ… **Beam search**: COW semantics for efficient forking
- âœ… **Flexible swapping**: Simulate CPU-GPU transfers
- âœ… **Comprehensive tests**: Unit tests and benchmarks included

## ğŸ“Š Benchmark Results

Run `python scripts/run_benchmarks.py` to see:

- Memory utilization comparison
- Throughput (tokens/sec) improvements
- Fragmentation reduction metrics
- Beam search memory savings
- Swap vs recompute tradeoffs

## ğŸ§ª Example Usage

```python
from paged_attention import PagedAttention, PagedKVCache, BlockAllocator

# Initialize components
allocator = BlockAllocator(total_blocks=128, block_size=16, hidden_dim=512)
kv_cache = PagedKVCache(block_size=16, hidden_dim=512, allocator=allocator)
attention = PagedAttention(hidden_dim=512, num_heads=8, block_size=16)

# Use in inference
query = torch.randn(1, 1, 512)  # New token query
output = attention(query, kv_cache)
```

## ğŸ“ˆ Extensions & Future Work

- [ ] CUDA kernel implementation for fused block access
- [ ] Adaptive block sizing based on sequence distribution
- [ ] Quantization for swapped blocks
- [ ] Distributed inference support
- [ ] Asynchronous swap pipeline

## ğŸ“š References

- PagedAttention Paper: "Efficient Memory Management for Large Language Model Serving with PagedAttention"
- vLLM: https://github.com/vllm-project/vllm

## ğŸ“ License

MIT License - Free for research and educational purposes.

```

---
```

#####

varunrao.gd@
